{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Working with NetCDF files - Calculating yearly averages for climate set datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### First setup our configuration and Spark context"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import StringIO\n",
      "import zipfile\n",
      "import re\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "spark_home = '/opt/spark-1.3.0-bin-hadoop2.4'\n",
      "os.environ['SPARK_HOME'] = spark_home\n",
      "\n",
      "sys.path.append(os.path.join(spark_home, 'python'))\n",
      "sys.path.append(os.path.join(spark_home, 'bin'))\n",
      "sys.path.append(os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "\n",
      "from pyspark import SparkContext, SparkFiles, SparkConf\n",
      "\n",
      "datafile_path = '/media/bitbucket/pr_amon_BCSD_rcp26_r1i1p1_CONUS_bcc-csm1-1_202101-202512.nc'\n",
      "parameter = 'pr'\n",
      "timesteps = 12\n",
      "partitions = 8\n",
      "grid_chunk_size = 2000\n",
      "\n",
      "spark_config = SparkConf();\n",
      "spark_config.set('spark.driver.memory', '10g')\n",
      "spark_config.set('spark.akka.frameSize', 32)\n",
      "spark_config.set('spark.executor.memory', '4g')\n",
      "spark_config.set('spark.driver.maxResultSize', '4g')\n",
      "spark_config.set('spark.shuffle.memoryFraction', 0.6)\n",
      "spark_config.set('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
      "spark_config.set('spark.kryoserializer.buffer.max.mb', 1024)\n",
      "\n",
      "sc = SparkContext('spark://ulex:7077', 'n_timesteps_mean', conf=spark_config)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Extract timestep chunks and grid chunks from NetCDF file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from netCDF4 import Dataset\n",
      "\n",
      "data = Dataset(datafile_path)\n",
      "pr = data.variables[parameter]\n",
      "\n",
      "# Get the number of timesteps\n",
      "num_timesteps = data.variables['time'].size\n",
      "\n",
      "# Get number of locations per timestep\n",
      "shape = pr[0].shape\n",
      "num_grid_points = pr[0].size\n",
      "\n",
      "# Break timesteps into n size chunks\n",
      "timestep_chunks = []\n",
      "for x in xrange(0, num_timesteps, timesteps):\n",
      "    if x + timesteps < num_timesteps:\n",
      "        timestep_chunks.append((x, x + timesteps))\n",
      "    else:\n",
      "        timestep_chunks.append((x, num_timesteps))\n",
      "\n",
      "# Break locations into chunks\n",
      "grid_chunks = []\n",
      "for lat in xrange(0, shape[0], grid_chunk_size):\n",
      "    for lon in xrange(0, shape[1], grid_chunk_size):\n",
      "        grid_chunks.append((lat, lon))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Now parallelize the grid chunks across the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_chunks = sc.parallelize(grid_chunks, partitions)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define function to calculate the mean for a given grid chunk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_means(grid_chunk):\n",
      "\n",
      "    data = Dataset(datafile_path)\n",
      "    pr = data.variables[parameter]\n",
      "\n",
      "    (lat, lon) = grid_chunk\n",
      "\n",
      "    values = []\n",
      "    for timestep_range in timestep_chunks:\n",
      "        (start_timesteps, end_timesteps) = timestep_range\n",
      "\n",
      "        mean = np.mean(pr[start_timesteps:end_timesteps,\n",
      "                          lat:lat+grid_chunk_size,\n",
      "                          lon:lon+grid_chunk_size], axis=0)\n",
      "        values.append(mean)\n",
      "\n",
      "    return values\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Calculate the yearly means for each chunk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "means = grid_chunks.map(calculate_means)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finally collect the chunks and recreate the grid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "means = means.collect()\n",
      "\n",
      "timestep_means = [np.ma.empty(shape) for x in range(len(timestep_chunks))]\n",
      "\n",
      "i = 0\n",
      "for lat in xrange(0, shape[0], grid_chunk_size):\n",
      "    for lon in xrange(0, shape[1], grid_chunk_size):\n",
      "        for j in range(len(timestep_chunks)):\n",
      "            chunk = means[i][j]\n",
      "            timestep_means[j][lat:lat+chunk.shape[0], lon:lon+chunk.shape[1]] = chunk\n",
      "\n",
      "        i += 1\n",
      "\n",
      "for m in timestep_means:\n",
      "        print(m[~m.mask])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2.957010049916183e-05 2.9542837485981483e-05 2.951173276718085e-05 ...,\n",
        " 1.878620605566539e-05 1.884244678270382e-05 1.8830543316047017e-05]\n",
        "[2.8391329882045586e-05 2.8350747015792876e-05 2.825960594539841e-05 ...,\n",
        " 1.850248736445792e-05 1.8515632594547544e-05 1.8503381094584864e-05]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[3.375462013840055e-05 3.367043245816603e-05 3.360415818557764e-05 ...,\n",
        " 1.7519595227592315e-05 1.751458694343455e-05 1.7499485693406314e-05]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[2.4524049270742882e-05 2.4500026484020054e-05 2.444009199583282e-05 ...,\n",
        " 1.7043377738445997e-05 1.706103648757562e-05 1.7049323408476386e-05]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[2.909944790493076e-05 2.9074484094356496e-05 2.903968803972627e-05 ...,\n",
        " 2.3176815981666248e-05 2.3199606706233073e-05 2.3192110044571262e-05]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Shutdown context ( we can only have one running at once )"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Using cluster wide accumulators - Calculate average elevation in North America using Shuttle Radar Topography Mission (SRTM) data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setup our Spark context"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import StringIO\n",
      "import zipfile\n",
      "import re\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "spark_home = '/opt/spark-1.3.0-bin-hadoop2.4'\n",
      "os.environ['SPARK_HOME'] = spark_home\n",
      "\n",
      "sys.path.append(os.path.join(spark_home, 'python'))\n",
      "sys.path.append(os.path.join(spark_home, 'bin'))\n",
      "sys.path.append(os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "\n",
      "from pyspark import SparkContext\n",
      "\n",
      "sc = SparkContext('spark://ulex:7077', 'srtm')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### First define the functions that will run on each node in the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "srtm_dtype = np.dtype('>i2')\n",
      "filename_regex = re.compile('([NSEW]\\d+[NSEW]\\d+).*')\n",
      "\n",
      "# Function to array\n",
      "def read_array(data):\n",
      "    hgt_2darray = np.flipud(np.fromstring(data, dtype=srtm_dtype).reshape(1201, 1201))\n",
      "\n",
      "    return hgt_2darray\n",
      "\n",
      "# Function to process a HGT file\n",
      "def process_file(file):\n",
      "    (name, content) = file\n",
      "\n",
      "    filename = os.path.basename(name)\n",
      "    srtm_name = filename.split('.')[0]\n",
      "    match = filename_regex.match(srtm_name)\n",
      "\n",
      "    # Skip anything that doesn't match\n",
      "    if not match:\n",
      "        return\n",
      "\n",
      "    hgt_file = '%s.hgt' % match.group(1)\n",
      "\n",
      "    stream = StringIO.StringIO(content)\n",
      "    try:\n",
      "        with zipfile.ZipFile(stream, 'r') as zipfd:\n",
      "            hgt_data = zipfd.read(hgt_file)\n",
      "            data = read_array(hgt_data)\n",
      "            samples = 0\n",
      "            sum = 0\n",
      "            for x in np.nditer(data):\n",
      "                if x != -32768:\n",
      "                    samples += 1\n",
      "                    sum += x\n",
      "\n",
      "            # Add the the local results to the global accumulators\n",
      "            num_samples_acc.add(samples)\n",
      "            sum_acc.add(sum)\n",
      "    except zipfile.BadZipfile:\n",
      "        # Skip anything thats not a zip\n",
      "        pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Now setup our global accumulators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_samples_acc = sc.accumulator(0)\n",
      "sum_acc = sc.accumulator(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load the data files accross the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_files = '/media/bitbucket/srtm/version2_1/SRTM3/North_America'\n",
      "data = sc.binaryFiles(data_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Process the files accross the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.foreach(process_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finally calculate the mean using the global accumulators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_acc.value / num_samples_acc.value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "544"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO Add lessons learnt etc. ...."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}
